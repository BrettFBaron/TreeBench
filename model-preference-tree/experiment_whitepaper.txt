# Measuring Mode Collapse in Large Language Models: A Multi-Level Decision Tree Approach

## Abstract

This whitepaper describes an experimental framework for measuring and quantifying mode collapse in large language models (LLMs) using a structured multi-level decision tree approach. By presenting models with consistent two-stage decision scenarios and measuring their response distributions using rigorous statistical metrics (Gini coefficient and Shannon entropy), we gain insights into how consistently LLMs gravitate toward specific choices. The experiment reveals patterns of preferential selection at different decision levels, providing a quantitative foundation for understanding model biases and behavioral tendencies.

## 1. Introduction

### 1.1 Problem Statement

As large language models become increasingly integrated into decision support systems, understanding their tendency to exhibit "mode collapse" – the phenomenon where a model repeatedly produces similar outputs despite having access to a broader space of valid responses – becomes crucial. This behavior can manifest as systematic biases or preferences for particular narrative paths, potentially limiting the diversity of outputs and skewing human decisions through subtle steering.

### 1.2 Research Objectives

This experiment aims to:

1. Quantify the degree of mode collapse in various LLMs
2. Compare mode collapse tendencies across different decision levels
3. Establish a standardized methodology for measuring model response diversity
4. Provide a foundation for comparing and benchmarking different models' preference patterns

## 2. Experimental Design

### 2.1 Two-Level Decision Tree

The experiment employs a structured two-level decision tree:

**Level 0 (Root)**: Models are asked to select a vacation destination country from five options:
- France
- Japan 
- Brazil
- Australia
- Italy

**Level 1 (Follow-up)**: Based on the country selected, models then choose an activity at that destination from five options:
- Museum
- National Park
- Beach
- High-end Restaurant
- Nightclub

The prompt is presented in a narrative format, asking the model to write a conversation between two friends planning a trip who need to select one destination and later one activity.

### 2.2 Option Randomization

To eliminate position bias, options are randomly shuffled for each sample. This controls for any potential preferences models might have for options presented first or last in a list.

### 2.3 Sampling

We collect 32 samples for each level:
- 32 samples at Level 0 (destination choice)
- 32 samples per active Level 0 choice at Level 1 (activity choice)

This substantial sample size ensures statistical robustness while allowing for the detection of subtle preferential patterns.

## 3. Metrics for Measuring Mode Collapse

We employ two complementary mathematical metrics to quantify mode collapse:

### 3.1 Gini Coefficient

The Gini coefficient is a measure of statistical dispersion commonly used to quantify inequality in a distribution. In our context:

- **Definition**: G = (2∑ᵢiYᵢ/n∑ᵢYᵢ) - (n+1)/n
  where i is the rank and Yᵢ is the value (frequency of choice)
  
- **Range**: 0 (perfect equality) to 1 (perfect inequality)
  
- **Interpretation**:
  - 0.0-0.3: Low mode collapse (choices distributed relatively evenly)
  - 0.3-0.6: Moderate mode collapse
  - 0.6-1.0: High mode collapse (strongly concentrated on few options)

The Gini coefficient is most sensitive to the dominant modes in the distribution, making it excellent for detecting when a model strongly prefers certain options.

### 3.2 Normalized Shannon Entropy

Shannon entropy measures the unpredictability or information content in a distribution:

- **Definition**: H(X) = -∑p(x)log₂p(x) / log₂(n)
  where p(x) is the probability of choice x, and n is the number of possible choices
  
- **Range**: 0 (minimum entropy) to 1 (maximum entropy)
  
- **Interpretation**:
  - 0.7-1.0: Low mode collapse (high diversity in responses)
  - 0.4-0.7: Moderate mode collapse
  - 0.0-0.4: High mode collapse (very predictable responses)

Entropy is more sensitive to the presence of rare choices, providing a complementary perspective to the Gini coefficient.

## 4. Implementation

### 4.1 Technical Architecture

The experiment is implemented as a web application with:

- A backend API that processes model responses and calculates metrics
- A database that stores responses and categorizations
- A tree structure to organize multi-level decisions
- A visualization interface for exploring results

### 4.2 Calculation of Metrics

For each model and each decision level:

1. Responses are categorized into the available options
2. Frequencies are converted to probability distributions
3. Gini coefficient and Shannon entropy are calculated
4. Metrics are normalized for comparison
5. Results are aggregated across levels

### 4.3 Mode Collapse Analysis

The system calculates metrics at three levels:
1. **Level 0**: Measuring mode collapse in destination selection
2. **Level 1**: Measuring mode collapse in activity selection for each destination
3. **Aggregate**: Combining metrics across levels to produce overall mode collapse scores

## 5. Results Interpretation

### 5.1 Comparative Analysis

Different patterns of results reveal distinctive model behaviors:

- **High Gini + Low Entropy at Level 0, Lower Gini + Higher Entropy at Level 1**:  
  Model has strong country preferences but demonstrates diversity in activities.

- **Similar Gini + Entropy at Both Levels**:  
  Model exhibits consistent decision-making patterns across both choice stages.

- **Low Overall Metrics**:  
  Model demonstrates high diversity and little mode collapse across all decisions.

### 5.2 Visualization

Results are presented through:
- Tabular comparisons of metrics by level
- Color-coded indicators of mode collapse severity
- Detailed breakdowns for each model

## 6. Implications and Applications

### 6.1 Model Development

These metrics provide valuable feedback for model developers to:
- Identify unintended biases in model outputs
- Track how fine-tuning affects response diversity
- Benchmark their models against others in terms of output diversity

### 6.2 Deployment Considerations

Organizations deploying LLMs can use these metrics to:
- Select models with appropriate levels of consistency/diversity for specific use cases
- Understand the potential for AI systems to artificially narrow human decision-making
- Implement mitigations for unwanted mode collapse in production systems

## 7. Conclusion

This experimental framework provides a structured, quantitative approach to measuring mode collapse in large language models. By using complementary statistical metrics (Gini coefficient and Shannon entropy) across multiple decision levels, we gain insights into how models navigate structured choice scenarios.

The approach reveals not just if a model exhibits mode collapse, but at which decision levels and to what degree. This nuanced understanding is crucial for the responsible development and deployment of language models in contexts where diversity of outputs and avoidance of unintended steering are important.

Future work will extend this methodology to more complex decision trees, additional metrics, and domain-specific applications.

---

© 2025 Model Preference Tree Project
https://model-preference-tree-35348ec25424.herokuapp.com/x